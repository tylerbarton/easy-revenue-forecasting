{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revenue Forecasting\n",
    "This notebook is used as an analytical platform to demonstrate price forecasting of future sales for 50 items across 10 stores. The approaches used vary in complexity to explore different approaches and compare the results.\n",
    "\n",
    "No approach in this notebook is carried out to the fullest extent that should be applied for actual business analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Modeling\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading The Data\n",
    "The data used herein has been obtained through Kaggle's [\"Store Item Demand Forecasting Challenge\"](https://www.kaggle.com/competitions/demand-forecasting-kernels-only/data). This data spans 5-years of sales data and will be used to predict future sales. The source of the data is not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "train = pd.read_csv('./data/train.csv', parse_dates=['date'], index_col=['date'])\n",
    "test = pd.read_csv('./data/test.csv', parse_dates=['date'], index_col=['date'])\n",
    "\n",
    "df = train.copy()\n",
    "\n",
    "# Display basic information\n",
    "print(train.shape, test.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing & Feature Engineering\n",
    "Using the information already given, we can expand fields to further look at trends such as sales by day of the week or month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General descriptors\n",
    "print(\"Date Range:\", train.index.min(), \" to \", train.index.max())\n",
    "\n",
    "sales_avg = train.sales.mean()\n",
    "print(\"Average Item Sales:\", sales_avg)\n",
    "\n",
    "# Feature Engineering - Expand date information\n",
    "df['day'] = df.index.day\n",
    "df['month'] = df.index.month\n",
    "df['year'] = df.index.year\n",
    "df['dayofweek'] = df.index.dayofweek\n",
    "df['quarter'] = ((df['month']+2)/3).astype(int)\n",
    "\n",
    "df.head(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyzing the Data\n",
    "Understanding the data is one of the most important things a data analyst can do. Looking for trends in the data may give insight on how to approach finding the answer you are looking for. Better yet, it could give you the answer you're looking for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of sales\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i in range(1,11):\n",
    "    if i < 6:\n",
    "        # Top Row\n",
    "        train[train.store == i].sales.hist(ax=axes[0, i-1])\n",
    "        axes[0, i-1].set_title(\"Store \" + str(i), fontsize = 15)\n",
    "    else:\n",
    "        # Bottom Row\n",
    "        train[train.store == i].sales.hist(ax=axes[1, i - 6])\n",
    "        axes[1, i-6].set_title(\"Store \" + str(i), fontsize = 15)\n",
    "plt.tight_layout(pad=3)\n",
    "plt.suptitle(\"Item Sales by Store\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms of item sales per store are nearly identical and suggest to me that this is either a synthetic dataset or an extreme relationship exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Relations\n",
    "There are many relations that can be explored:\n",
    " - item-store relationship\n",
    " - seasonal (or quarter) relationship\n",
    " - days of the week relationship\n",
    " - changes by year or month\n",
    "\n",
    "Each could provide different and unique insights into the trends we are looking to explore. Visualizing this data may even extrapolate the trend right here and now to give us a quick answer. However, Only a few of these relationships will be demonstrated as a proof of concept. The rest is left to the reader as an exercise.\n",
    "\n",
    "### Item-Store relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_item_store = pd.pivot_table(df, index='store', columns='item',\n",
    "                                values='sales', aggfunc=np.mean).values\n",
    "# print(item_store_table)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Items\n",
    "plt.subplot(122)\n",
    "plt.plot(table_item_store / table_item_store.mean(0)[np.newaxis])\n",
    "plt.title(\"Items\")\n",
    "plt.xlabel(\"Store\")\n",
    "plt.ylabel(\"Sales Relative to Average\")\n",
    "\n",
    "# Stores\n",
    "plt.subplot(131)\n",
    "plt.plot(table_item_store.T / table_item_store.T.mean(0)[np.newaxis])\n",
    "plt.title(\"Stores\")\n",
    "plt.xlabel(\"Item\")\n",
    "plt.ylabel(\"Sales Relative to Average\")\n",
    "plt.legend(labels=df['store'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Date relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_item_dow = pd.pivot_table(df, index='dayofweek', columns='item',\n",
    "                                values='sales', aggfunc=np.mean).values\n",
    "table_item_year = pd.pivot_table(df, index='year', columns='item',\n",
    "                               values='sales', aggfunc=np.mean).values\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Day of the week\n",
    "week_days = ['', 'Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.set_xticklabels(week_days)\n",
    "plt.plot(table_item_dow / table_item_dow.mean(0)[np.newaxis])\n",
    "plt.title(\"Days of the Week\")\n",
    "plt.xlabel(\"Day of the Week\")\n",
    "plt.ylabel(\"Sales Relative to Average\")\n",
    "\n",
    "# Years\n",
    "years = np.arange(2012.5,2018.5, 0.5)\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.set_xticklabels(years)\n",
    "plt.plot(table_item_year / table_item_year.mean(0)[np.newaxis])\n",
    "plt.title(\"Years\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Sales Relative to Average\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear to see by the graphs that items and stores are performing without degeneracies in the data.\n",
    "\n",
    "*note*: The warning produced by the graphs is due to a [bug.](https://github.com/pandas-dev/pandas/issues/35684) However, in order to provide more clarity, I choose to keep the warning and display the x labels more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Simple Linear Regression\n",
    "Using an approach least-squares polynomial fit to predict future yearly sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_table = pd.pivot_table(df, index='year', values='sales', aggfunc=np.mean)\n",
    "year_table = year_table / sales_avg\n",
    "\n",
    "years = np.arange(2013, 2019)\n",
    "annual_sales_avg = year_table.values.squeeze()\n",
    "\n",
    "# Perform polynomial fits\n",
    "p1 = np.poly1d(np.polyfit(years[:-1], annual_sales_avg, 1))\n",
    "p2 = np.poly1d(np.polyfit(years[:-1], annual_sales_avg, 2))\n",
    "\n",
    "print(\"Relative Sales in 2018 based on Degree-1 (Linear) Fit:  \", \"{:.4f}\".format(p1(2018)))\n",
    "print(\"Relative Sales in 2018 based on Degree-2 (Quadratic) Fit:\", \"{:.4f}\".format(p2(2018)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.plot(years[:-1], annual_sales_avg, 'o')\n",
    "plt.plot(years, p1(years), 'C0')\n",
    "plt.plot(years, p2(years), 'C1')\n",
    "\n",
    "plt.title(\"Relative Sales by Year\")\n",
    "plt.legend([\"Annual Sales\", \"Linear fit\", \"Quadratic fit\"])\n",
    "plt.xlim(2012.5, 2018.5)\n",
    "plt.ylabel(\"Relative Sales\")\n",
    "plt.xlabel(\"Year\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the quadratic fit more-closely follows the trend. Hence, the quadratic fit would be more accurate over the linear fit in the prediction of the 2018 season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Advanced Prediction - LGBM\n",
    "Light Gradient boosting is high-performance gradient boosting for machine learning tasks dealing with either large or small data-sets. Here, we are going to train a model based on sales data in an attempt to better predict future sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partitioning\n",
    "Start by splitting and formatting the training and testing sets.\n",
    "\n",
    "*note*: Test set is interchangably used with validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by data for time series split\n",
    "df = df.sort_index()\n",
    "\n",
    "# Train-Validation Split\n",
    "train = df.loc[(df.index < \"2017-10-01\"), :]\n",
    "val = df.loc[(df.index >= \"2017-10-01\") & (df.index < \"2018-01-01\"), :]\n",
    "\n",
    "train_features = [col for col in train.columns if col not in ['sales', 'year']]\n",
    "\n",
    "X_train = train[train_features]\n",
    "Y_train = train['sales']\n",
    "X_val = val[train_features]\n",
    "Y_val = val['sales']\n",
    "\n",
    "train_dataset = lgb.Dataset(X_train, Y_train)\n",
    "test_dataset = lgb.Dataset(X_val, Y_val)\n",
    "\n",
    "print(Y_train.shape, X_train.shape, Y_val.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have our training and validation sets, we will configure LGBM to train the model. \n",
    "\n",
    "Explaining every detail of this process is a little out of the scope of this simplified approach. One could change the loss function, the number of iterations, the learning rate, among the myriad of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE function\n",
    "def mean_absolute_error(preds, data):\n",
    "    actuals = data.get_label()\n",
    "    err = (actuals - preds).sum()\n",
    "    is_higher_better = False\n",
    "    return \"MAE\", err, is_higher_better\n",
    "\n",
    "\n",
    "# LGBM Configuration\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"mae\",\n",
    "    \"learning_rate\": 0.7,\n",
    "    \"force_row_wise\": True,\n",
    "}\n",
    "\n",
    "rounds = 15\n",
    "\n",
    "# Train the model\n",
    "booster = lgb.train(params,\n",
    "                    feval=mean_absolute_error,\n",
    "                    train_set=train_dataset,\n",
    "                    valid_sets=(test_dataset,),\n",
    "                    num_boost_round=rounds\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Error Rate\n",
    "It is important to see how the models performed on the training & test sets. Ideally, the model should get 100% accuracy on the training set and a very high accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy of our model\n",
    "test_preds = booster.predict(X_val)\n",
    "train_preds = booster.predict(X_train)\n",
    "\n",
    "print(\"\\nTrain R2 Score :\", \"{:.2f}\".format(r2_score(Y_train, train_preds)))\n",
    "print(\"Test  R2 Score :\", \"{:.2f}\".format(r2_score(Y_val, test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_preds).describe([0.1, 0.75, 0.8, 0.9, 0.95, 0.99]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring Out What Parameters Mattered\n",
    "In real life, we want to see which parameters are having the biggest impact on the value we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "lgb.plot_importance(booster, importance_type=\"gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "While it would be nice to use one-size-fits-all algorithms on data, it is necessary to understand the data that is being interpreted. A guess that this is synthetic data would have us using different methods than real-world data and even then, the nature of that data would each require unique solutions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
